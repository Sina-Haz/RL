{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7a2a713-3ad3-4833-8c5a-004e24fe9a7a",
   "metadata": {},
   "source": [
    "### Unit 1 Assignment\n",
    "\n",
    "Train an agent for the Lunar-Landerv2 environment using Stable-Baselines3 library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b2c6486-4917-4339-8380-1826c5e2995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PATH'] += '/common/home/skh79/local/xvfb/bin/Xvfb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "302fcf93-db90-4c27-a3ad-2eaf15228541",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Xvfb'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Virtual display\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyvirtualdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Display\n\u001b[0;32m----> 4\u001b[0m virtual_display \u001b[38;5;241m=\u001b[39m \u001b[43mDisplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvisible\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m900\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m virtual_display\u001b[38;5;241m.\u001b[39mstart()\n",
      "File \u001b[0;32m~/miniconda3/envs/pkgenv/lib/python3.9/site-packages/pyvirtualdisplay/display.py:54\u001b[0m, in \u001b[0;36mDisplay.__init__\u001b[0;34m(self, backend, visible, size, color_depth, bgcolor, use_xauth, retries, extra_args, manage_global_env, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcls\u001b[39m:\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munknown backend: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend)\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolor_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolor_depth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbgcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbgcolor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_xauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_xauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# check_startup=check_startup,\u001b[39;49;00m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmanage_global_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanage_global_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pkgenv/lib/python3.9/site-packages/pyvirtualdisplay/xvfb.py:44\u001b[0m, in \u001b[0;36mXvfbDisplay.__init__\u001b[0;34m(self, size, color_depth, bgcolor, use_xauth, fbdir, dpi, retries, extra_args, manage_global_env)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fbdir \u001b[38;5;241m=\u001b[39m fbdir\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dpi \u001b[38;5;241m=\u001b[39m dpi\n\u001b[0;32m---> 44\u001b[0m \u001b[43mAbstractDisplay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mPROGRAM\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_xauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_xauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mextra_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmanage_global_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanage_global_env\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pkgenv/lib/python3.9/site-packages/pyvirtualdisplay/abstractdisplay.py:85\u001b[0m, in \u001b[0;36mAbstractDisplay.__init__\u001b[0;34m(self, program, use_xauth, retries, extra_args, manage_global_env)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pipe_wfd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retries_current \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 85\u001b[0m helptext \u001b[38;5;241m=\u001b[39m \u001b[43mget_helptext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprogram\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_displayfd \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-displayfd\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m helptext\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_displayfd:\n",
      "File \u001b[0;32m~/miniconda3/envs/pkgenv/lib/python3.9/site-packages/pyvirtualdisplay/util.py:13\u001b[0m, in \u001b[0;36mget_helptext\u001b[0;34m(program)\u001b[0m\n\u001b[1;32m      6\u001b[0m cmd \u001b[38;5;241m=\u001b[39m [program, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-help\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# py3.7+\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# p = subprocess.run(cmd, capture_output=True)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# stderr = p.stderr\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# py3.6 also\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstderr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshell\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m _, stderr \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mcommunicate()\n\u001b[1;32m     21\u001b[0m helptext \u001b[38;5;241m=\u001b[39m stderr\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pkgenv/lib/python3.9/subprocess.py:951\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_mode:\n\u001b[1;32m    948\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[1;32m    949\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m--> 951\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    961\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n\u001b[1;32m    962\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mfilter\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdin, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstdout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr)):\n",
      "File \u001b[0;32m~/miniconda3/envs/pkgenv/lib/python3.9/subprocess.py:1837\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001b[0m\n\u001b[1;32m   1835\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errno_num \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1836\u001b[0m         err_msg \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mstrerror(errno_num)\n\u001b[0;32m-> 1837\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(errno_num, err_msg, err_filename)\n\u001b[1;32m   1838\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m child_exception_type(err_msg)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Xvfb'"
     ]
    }
   ],
   "source": [
    "# Virtual display\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62e1fb80-4c00-4669-964d-d8cf47c70ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "\n",
    "from huggingface_sb3 import load_from_hub, package_to_hub\n",
    "from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b522d921-7508-4a10-a6d2-aa43a3a3544d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action taken: 0\n",
      "Action taken: 2\n",
      "Action taken: 3\n",
      "Action taken: 0\n",
      "Action taken: 0\n",
      "Action taken: 0\n",
      "Action taken: 0\n",
      "Action taken: 0\n",
      "Action taken: 2\n",
      "Action taken: 1\n",
      "Action taken: 1\n",
      "Action taken: 1\n",
      "Action taken: 0\n",
      "Action taken: 3\n",
      "Action taken: 1\n",
      "Action taken: 3\n",
      "Action taken: 2\n",
      "Action taken: 1\n",
      "Action taken: 2\n",
      "Action taken: 0\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# First, we create our environment called LunarLander-v2\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "# Then we reset this environment\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(20):\n",
    "  # Take a random action\n",
    "  action = env.action_space.sample()\n",
    "  print(\"Action taken:\", action)\n",
    "\n",
    "  # Do this action in the environment and get\n",
    "  # next_state, reward, terminated, truncated and info\n",
    "  observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "  # If the game is terminated (in our case we land, crashed) or truncated (timeout)\n",
    "  if terminated or truncated:\n",
    "      # Reset the environment\n",
    "      print(\"Environment is reset\")\n",
    "      observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fb36ea7-1df9-4819-9ab4-7ce452787097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____OBSERVATION SPACE_____ \n",
      "\n",
      "Observation Space Shape (8,)\n",
      "Sample observation [ 4.2897957e+01 -1.6988243e+01  3.0034523e+00  2.1834114e+00\n",
      " -1.6388742e+00  3.7806518e+00  6.5736181e-01  2.0748030e-02]\n"
     ]
    }
   ],
   "source": [
    "# We create our environment with gym.make(\"<name_of_the_environment>\")\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env.reset()\n",
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"Observation Space Shape\", env.observation_space.shape)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c8566d-47b8-415b-bb81-262ebea0fc2b",
   "metadata": {},
   "source": [
    "We see with Observation Space Shape (8,) that the observation is a vector of size 8, where each value contains different information about the lander:\n",
    "-  Horizontal pad coordinate (x)\n",
    "- Vertical pad coordinate (y)\n",
    "- Horizontal speed (x)\n",
    "- Vertical speed (y)\n",
    "- Angle\n",
    "- Angular speed\n",
    "- If the left leg contact point has touched the land (boolean)\n",
    "- If the right leg contact point has touched the land (boolean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e46aabc0-8f12-4a9a-9993-4e4adebe374e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " _____ACTION SPACE_____ \n",
      "\n",
      "Action Space Shape 4\n",
      "Action Space Sample 3\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"Action Space Shape\", env.action_space.n)\n",
    "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e8c3c0-dd1e-4b39-8d7e-8580c4bdc22f",
   "metadata": {},
   "source": [
    "The action space (the set of possible actions the agent can take) is discrete with 4 actions available ðŸŽ®:\n",
    "\n",
    "- Action 0: Do nothing,\n",
    "- Action 1: Fire left orientation engine,\n",
    "- Action 2: Fire the main engine,\n",
    "- Action 3: Fire right orientation engine.\n",
    "\n",
    "Reward function (the function that will give a reward at each timestep) ðŸ’°:\n",
    "\n",
    "After every step a reward is granted. The total reward of an episode is the **sum of the rewards for all the steps within that episode**.\n",
    "\n",
    "For each step, the reward:\n",
    "\n",
    "- Is increased/decreased the closer/further the lander is to the landing pad.\n",
    "-  Is increased/decreased the slower/faster the lander is moving.\n",
    "- Is decreased the more the lander is tilted (angle not horizontal).\n",
    "- Is increased by 10 points for each leg that is in contact with the ground.\n",
    "- Is decreased by 0.03 points each frame a side engine is firing.\n",
    "- Is decreased by 0.3 points each frame the main engine is firing.\n",
    "\n",
    "The episode receive an **additional reward of -100 or +100 points for crashing or landing safely respectively.**\n",
    "\n",
    "An episode is **considered a solution if it scores at least 200 points.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2718792a-3096-44c9-9f95-148ddcedcda4",
   "metadata": {},
   "source": [
    "#### Vectorized Environment\n",
    "\n",
    "- We create a vectorized environment (a method for stacking multiple independent environments into a single environment) of 16 environments, this way, **we'll have more diverse experiences during the training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a1251a7-6e96-4d97-85b4-f5fcc55dbafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = make_vec_env('LunarLander-v2', n_envs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9463a42f-4422-496c-ae49-ec9024729255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mPPO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpolicy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstable_baselines3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mActorCriticPolicy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0menv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgymnasium\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstable_baselines3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_vec_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVecEnv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlearning_rate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0003\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_steps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_epochs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mgamma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mgae_lambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mclip_range\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mclip_range_vf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNoneType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnormalize_advantage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0ment_coef\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mvf_coef\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0muse_sde\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msde_sample_freq\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtarget_kl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstats_window_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtensorboard_log\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpolicy_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0m_init_setup_model\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Proximal Policy Optimization algorithm (PPO) (clip version)\n",
       "\n",
       "Paper: https://arxiv.org/abs/1707.06347\n",
       "Code: This implementation borrows code from OpenAI Spinning Up (https://github.com/openai/spinningup/)\n",
       "https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail and\n",
       "Stable Baselines (PPO2 from https://github.com/hill-a/stable-baselines)\n",
       "\n",
       "Introduction to PPO: https://spinningup.openai.com/en/latest/algorithms/ppo.html\n",
       "\n",
       ":param policy: The policy model to use (MlpPolicy, CnnPolicy, ...)\n",
       ":param env: The environment to learn from (if registered in Gym, can be str)\n",
       ":param learning_rate: The learning rate, it can be a function\n",
       "    of the current progress remaining (from 1 to 0)\n",
       ":param n_steps: The number of steps to run for each environment per update\n",
       "    (i.e. rollout buffer size is n_steps * n_envs where n_envs is number of environment copies running in parallel)\n",
       "    NOTE: n_steps * n_envs must be greater than 1 (because of the advantage normalization)\n",
       "    See https://github.com/pytorch/pytorch/issues/29372\n",
       ":param batch_size: Minibatch size\n",
       ":param n_epochs: Number of epoch when optimizing the surrogate loss\n",
       ":param gamma: Discount factor\n",
       ":param gae_lambda: Factor for trade-off of bias vs variance for Generalized Advantage Estimator\n",
       ":param clip_range: Clipping parameter, it can be a function of the current progress\n",
       "    remaining (from 1 to 0).\n",
       ":param clip_range_vf: Clipping parameter for the value function,\n",
       "    it can be a function of the current progress remaining (from 1 to 0).\n",
       "    This is a parameter specific to the OpenAI implementation. If None is passed (default),\n",
       "    no clipping will be done on the value function.\n",
       "    IMPORTANT: this clipping depends on the reward scaling.\n",
       ":param normalize_advantage: Whether to normalize or not the advantage\n",
       ":param ent_coef: Entropy coefficient for the loss calculation\n",
       ":param vf_coef: Value function coefficient for the loss calculation\n",
       ":param max_grad_norm: The maximum value for the gradient clipping\n",
       ":param use_sde: Whether to use generalized State Dependent Exploration (gSDE)\n",
       "    instead of action noise exploration (default: False)\n",
       ":param sde_sample_freq: Sample a new noise matrix every n steps when using gSDE\n",
       "    Default: -1 (only sample at the beginning of the rollout)\n",
       ":param target_kl: Limit the KL divergence between updates,\n",
       "    because the clipping is not enough to prevent large update\n",
       "    see issue #213 (cf https://github.com/hill-a/stable-baselines/issues/213)\n",
       "    By default, there is no limit on the kl div.\n",
       ":param stats_window_size: Window size for the rollout logging, specifying the number of episodes to average\n",
       "    the reported success rate, mean episode length, and mean reward over\n",
       ":param tensorboard_log: the log location for tensorboard (if None, no logging)\n",
       ":param policy_kwargs: additional arguments to be passed to the policy on creation\n",
       ":param verbose: Verbosity level: 0 for no output, 1 for info messages (such as device or wrappers used), 2 for\n",
       "    debug messages\n",
       ":param seed: Seed for the pseudo random generators\n",
       ":param device: Device (cpu, cuda, ...) on which the code should be run.\n",
       "    Setting it to auto, the code will be run on the GPU if possible.\n",
       ":param _init_setup_model: Whether or not to build the network at the creation of the instance\n",
       "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/envs/pkgenv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py\n",
       "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe713334-8b6b-48b6-a2b0-f331222a7b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af2c2c60-bd3c-4a04-be61-03c3e16d3516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Instantiate MlpPolicy model so that it works with vectorized environment env\n",
    "model = PPO(\n",
    "    policy = 'MlpPolicy',\n",
    "    env = env,\n",
    "    n_steps = 1024,\n",
    "    batch_size = 64,\n",
    "    n_epochs = 4,\n",
    "    gamma = 0.9995,\n",
    "    gae_lambda = 0.98,\n",
    "    ent_coef = 0.01,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f681f7a5-2199-47f0-b441-c27b21994796",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 96.2     |\n",
      "|    ep_rew_mean     | -204     |\n",
      "| time/              |          |\n",
      "|    fps             | 2876     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 5        |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 94           |\n",
      "|    ep_rew_mean          | -173         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1650         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.008593319  |\n",
      "|    clip_fraction        | 0.034        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | -0.000518322 |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.81e+03     |\n",
      "|    n_updates            | 4            |\n",
      "|    policy_gradient_loss | -0.0063      |\n",
      "|    value_loss           | 5.24e+03     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 91.5          |\n",
      "|    ep_rew_mean          | -125          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1439          |\n",
      "|    iterations           | 3             |\n",
      "|    time_elapsed         | 34            |\n",
      "|    total_timesteps      | 49152         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0070143174  |\n",
      "|    clip_fraction        | 0.0674        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.37         |\n",
      "|    explained_variance   | -0.0047262907 |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.33e+03      |\n",
      "|    n_updates            | 8             |\n",
      "|    policy_gradient_loss | -0.00597      |\n",
      "|    value_loss           | 3.4e+03       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 106           |\n",
      "|    ep_rew_mean          | -120          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1332          |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 49            |\n",
      "|    total_timesteps      | 65536         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0073851384  |\n",
      "|    clip_fraction        | 0.0483        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.36         |\n",
      "|    explained_variance   | -0.0020239353 |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 312           |\n",
      "|    n_updates            | 12            |\n",
      "|    policy_gradient_loss | -0.00367      |\n",
      "|    value_loss           | 1.23e+03      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 100         |\n",
      "|    ep_rew_mean          | -112        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1291        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 63          |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009015147 |\n",
      "|    clip_fraction        | 0.0576      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.015619695 |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 185         |\n",
      "|    n_updates            | 16          |\n",
      "|    policy_gradient_loss | -0.00419    |\n",
      "|    value_loss           | 748         |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 98.9          |\n",
      "|    ep_rew_mean          | -110          |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1258          |\n",
      "|    iterations           | 6             |\n",
      "|    time_elapsed         | 78            |\n",
      "|    total_timesteps      | 98304         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.010697825   |\n",
      "|    clip_fraction        | 0.0814        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.34         |\n",
      "|    explained_variance   | 0.00044304132 |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 387           |\n",
      "|    n_updates            | 20            |\n",
      "|    policy_gradient_loss | -0.0024       |\n",
      "|    value_loss           | 905           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 110          |\n",
      "|    ep_rew_mean          | -101         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1304         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 87           |\n",
      "|    total_timesteps      | 114688       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.008854622  |\n",
      "|    clip_fraction        | 0.0559       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.32        |\n",
      "|    explained_variance   | -0.004787326 |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 341          |\n",
      "|    n_updates            | 24           |\n",
      "|    policy_gradient_loss | -0.00426     |\n",
      "|    value_loss           | 855          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 108          |\n",
      "|    ep_rew_mean          | -85.9        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1344         |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 97           |\n",
      "|    total_timesteps      | 131072       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.005126858  |\n",
      "|    clip_fraction        | 0.0317       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.32        |\n",
      "|    explained_variance   | -0.018590689 |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 303          |\n",
      "|    n_updates            | 28           |\n",
      "|    policy_gradient_loss | -0.00174     |\n",
      "|    value_loss           | 768          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 117           |\n",
      "|    ep_rew_mean          | -54.2         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1332          |\n",
      "|    iterations           | 9             |\n",
      "|    time_elapsed         | 110           |\n",
      "|    total_timesteps      | 147456        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0077845696  |\n",
      "|    clip_fraction        | 0.0737        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.27         |\n",
      "|    explained_variance   | -0.0006275177 |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 253           |\n",
      "|    n_updates            | 32            |\n",
      "|    policy_gradient_loss | -0.00334      |\n",
      "|    value_loss           | 580           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 134          |\n",
      "|    ep_rew_mean          | -35.1        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1292         |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 126          |\n",
      "|    total_timesteps      | 163840       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.010041063  |\n",
      "|    clip_fraction        | 0.0774       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.24        |\n",
      "|    explained_variance   | -0.010904074 |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 135          |\n",
      "|    n_updates            | 36           |\n",
      "|    policy_gradient_loss | -0.00536     |\n",
      "|    value_loss           | 353          |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 133           |\n",
      "|    ep_rew_mean          | -25.9         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1270          |\n",
      "|    iterations           | 11            |\n",
      "|    time_elapsed         | 141           |\n",
      "|    total_timesteps      | 180224        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.008031568   |\n",
      "|    clip_fraction        | 0.0567        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.21         |\n",
      "|    explained_variance   | 0.00024914742 |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 218           |\n",
      "|    n_updates            | 40            |\n",
      "|    policy_gradient_loss | -0.00466      |\n",
      "|    value_loss           | 364           |\n",
      "-------------------------------------------\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 135            |\n",
      "|    ep_rew_mean          | -14.6          |\n",
      "| time/                   |                |\n",
      "|    fps                  | 1248           |\n",
      "|    iterations           | 12             |\n",
      "|    time_elapsed         | 157            |\n",
      "|    total_timesteps      | 196608         |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 0.007000161    |\n",
      "|    clip_fraction        | 0.0479         |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -1.17          |\n",
      "|    explained_variance   | -0.00027132034 |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | 238            |\n",
      "|    n_updates            | 44             |\n",
      "|    policy_gradient_loss | -0.00287       |\n",
      "|    value_loss           | 510            |\n",
      "--------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 165           |\n",
      "|    ep_rew_mean          | -10.3         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1224          |\n",
      "|    iterations           | 13            |\n",
      "|    time_elapsed         | 173           |\n",
      "|    total_timesteps      | 212992        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.0076847393  |\n",
      "|    clip_fraction        | 0.0423        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.18         |\n",
      "|    explained_variance   | -0.0021246672 |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 292           |\n",
      "|    n_updates            | 48            |\n",
      "|    policy_gradient_loss | -0.00198      |\n",
      "|    value_loss           | 528           |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 234           |\n",
      "|    ep_rew_mean          | -1.57         |\n",
      "| time/                   |               |\n",
      "|    fps                  | 1199          |\n",
      "|    iterations           | 14            |\n",
      "|    time_elapsed         | 191           |\n",
      "|    total_timesteps      | 229376        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.009771      |\n",
      "|    clip_fraction        | 0.0773        |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -1.16         |\n",
      "|    explained_variance   | 8.4638596e-05 |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 255           |\n",
      "|    n_updates            | 52            |\n",
      "|    policy_gradient_loss | -0.00182      |\n",
      "|    value_loss           | 585           |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 312          |\n",
      "|    ep_rew_mean          | -7.59        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1161         |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 211          |\n",
      "|    total_timesteps      | 245760       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076105264 |\n",
      "|    clip_fraction        | 0.0466       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.15        |\n",
      "|    explained_variance   | 0.0049545765 |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 201          |\n",
      "|    n_updates            | 56           |\n",
      "|    policy_gradient_loss | -0.00245     |\n",
      "|    value_loss           | 512          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 353         |\n",
      "|    ep_rew_mean          | -18.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1120        |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 233         |\n",
      "|    total_timesteps      | 262144      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004764185 |\n",
      "|    clip_fraction        | 0.0302      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.17       |\n",
      "|    explained_variance   | 0.028220594 |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 251         |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00125    |\n",
      "|    value_loss           | 519         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 352          |\n",
      "|    ep_rew_mean          | -22.3        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1082         |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 257          |\n",
      "|    total_timesteps      | 278528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0031452035 |\n",
      "|    clip_fraction        | 0.0164       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.14        |\n",
      "|    explained_variance   | 0.09039992   |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 99.3         |\n",
      "|    n_updates            | 64           |\n",
      "|    policy_gradient_loss | -0.00189     |\n",
      "|    value_loss           | 496          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 406          |\n",
      "|    ep_rew_mean          | -19          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1036         |\n",
      "|    iterations           | 18           |\n",
      "|    time_elapsed         | 284          |\n",
      "|    total_timesteps      | 294912       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050560036 |\n",
      "|    clip_fraction        | 0.0245       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.13        |\n",
      "|    explained_variance   | 0.32632452   |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 255          |\n",
      "|    n_updates            | 68           |\n",
      "|    policy_gradient_loss | -0.00146     |\n",
      "|    value_loss           | 527          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 454         |\n",
      "|    ep_rew_mean          | -13.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1003        |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 310         |\n",
      "|    total_timesteps      | 311296      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005970928 |\n",
      "|    clip_fraction        | 0.0397      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 0.3965329   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 160         |\n",
      "|    n_updates            | 72          |\n",
      "|    policy_gradient_loss | -0.00193    |\n",
      "|    value_loss           | 418         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 520          |\n",
      "|    ep_rew_mean          | -7.98        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 973          |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 336          |\n",
      "|    total_timesteps      | 327680       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063852062 |\n",
      "|    clip_fraction        | 0.0672       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.16        |\n",
      "|    explained_variance   | 0.55587065   |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 92.3         |\n",
      "|    n_updates            | 76           |\n",
      "|    policy_gradient_loss | -0.00202     |\n",
      "|    value_loss           | 260          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 576         |\n",
      "|    ep_rew_mean          | 7.8         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 950         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 362         |\n",
      "|    total_timesteps      | 344064      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004242257 |\n",
      "|    clip_fraction        | 0.0318      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | 0.5713332   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 169         |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00251    |\n",
      "|    value_loss           | 322         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 637         |\n",
      "|    ep_rew_mean          | 25.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 929         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 387         |\n",
      "|    total_timesteps      | 360448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005686677 |\n",
      "|    clip_fraction        | 0.0472      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.7327176   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 62.8        |\n",
      "|    n_updates            | 84          |\n",
      "|    policy_gradient_loss | -0.0031     |\n",
      "|    value_loss           | 162         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 655         |\n",
      "|    ep_rew_mean          | 34.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 910         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 413         |\n",
      "|    total_timesteps      | 376832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006534105 |\n",
      "|    clip_fraction        | 0.0668      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.79116434  |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 31.7        |\n",
      "|    n_updates            | 88          |\n",
      "|    policy_gradient_loss | -0.0028     |\n",
      "|    value_loss           | 125         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 716          |\n",
      "|    ep_rew_mean          | 51.6         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 887          |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 443          |\n",
      "|    total_timesteps      | 393216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0072404635 |\n",
      "|    clip_fraction        | 0.0531       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.11        |\n",
      "|    explained_variance   | 0.84724593   |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 28.7         |\n",
      "|    n_updates            | 92           |\n",
      "|    policy_gradient_loss | -0.0019      |\n",
      "|    value_loss           | 117          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 778         |\n",
      "|    ep_rew_mean          | 69.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 868         |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 471         |\n",
      "|    total_timesteps      | 409600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005462381 |\n",
      "|    clip_fraction        | 0.0258      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.13       |\n",
      "|    explained_variance   | 0.9477972   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.03        |\n",
      "|    n_updates            | 96          |\n",
      "|    policy_gradient_loss | -0.00018    |\n",
      "|    value_loss           | 28.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 832          |\n",
      "|    ep_rew_mean          | 79.1         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 850          |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 501          |\n",
      "|    total_timesteps      | 425984       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0042634644 |\n",
      "|    clip_fraction        | 0.0216       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.12        |\n",
      "|    explained_variance   | 0.9387545    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 27.7         |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -0.00164     |\n",
      "|    value_loss           | 44.1         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 866          |\n",
      "|    ep_rew_mean          | 85.6         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 839          |\n",
      "|    iterations           | 27           |\n",
      "|    time_elapsed         | 526          |\n",
      "|    total_timesteps      | 442368       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066957395 |\n",
      "|    clip_fraction        | 0.0554       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.11        |\n",
      "|    explained_variance   | 0.96831083   |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 18.3         |\n",
      "|    n_updates            | 104          |\n",
      "|    policy_gradient_loss | -0.00112     |\n",
      "|    value_loss           | 24.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 898          |\n",
      "|    ep_rew_mean          | 95.3         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 827          |\n",
      "|    iterations           | 28           |\n",
      "|    time_elapsed         | 554          |\n",
      "|    total_timesteps      | 458752       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035294513 |\n",
      "|    clip_fraction        | 0.0121       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.12        |\n",
      "|    explained_variance   | 0.931231     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 25.3         |\n",
      "|    n_updates            | 108          |\n",
      "|    policy_gradient_loss | -0.000636    |\n",
      "|    value_loss           | 72.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 886          |\n",
      "|    ep_rew_mean          | 92.5         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 819          |\n",
      "|    iterations           | 29           |\n",
      "|    time_elapsed         | 579          |\n",
      "|    total_timesteps      | 475136       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0027020331 |\n",
      "|    clip_fraction        | 0.0108       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 0.9484944    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 38.8         |\n",
      "|    n_updates            | 112          |\n",
      "|    policy_gradient_loss | -0.000244    |\n",
      "|    value_loss           | 45.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 891          |\n",
      "|    ep_rew_mean          | 94.6         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 810          |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 606          |\n",
      "|    total_timesteps      | 491520       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058037555 |\n",
      "|    clip_fraction        | 0.0376       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 0.9379797    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 70           |\n",
      "|    n_updates            | 116          |\n",
      "|    policy_gradient_loss | -0.00183     |\n",
      "|    value_loss           | 72           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 884          |\n",
      "|    ep_rew_mean          | 95.4         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 800          |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 634          |\n",
      "|    total_timesteps      | 507904       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0041437675 |\n",
      "|    clip_fraction        | 0.023        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.9689357    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.45         |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00163     |\n",
      "|    value_loss           | 30.8         |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model_name = 'ppo-Lunar-Lander-v2'\n",
    "\n",
    "# do a small amount of training and run it to see how well we do\n",
    "model.learn(total_timesteps=500000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77bb5325-cdfb-420a-8bbf-e069b2ac1652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating our model in a new, not vectorized environment\n",
    "\n",
    "# We wrap the environment in a monitor so it can be displayed\n",
    "eval_env = Monitor(gym.make(\"LunarLander-v2\", render_mode='rgb_array'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9170e8c7-839a-4ad2-bf56-374b00820498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward=-12.38 +/- 21.713644556799206\n"
     ]
    }
   ],
   "source": [
    "# Stable-Baselines3 provides function evaluate_policy to check how well our model performs\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee000826-c7e6-4422-9972-267027ccd1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load('models/ppo-lander-curr.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa3b61a8-72f4-4cec-9c09-66d07c9a9c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can compare by loading our previous model:\n",
    "from huggingface_sb3 import load_from_hub\n",
    "checkpoint = load_from_hub(\n",
    "\trepo_id=\"shazeghi/ppo-LunarLander-v2\",\n",
    "\tfilename=\"ppo-LunarLander-v2.zip\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "178f9c4d-7c97-4a40-af71-e1cf0232ed17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7fddeda48fd0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_model = PPO.load(checkpoint, env=env)\n",
    "prev_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f720dce4-b1dd-492c-be8e-d0d2f11009e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward=-19.75 +/- 17.764647946642366\n",
      "mean_reward=224.81 +/- 32.502890864698415\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, env=eval_env):\n",
    "    mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "    print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")\n",
    "\n",
    "evaluate(model)\n",
    "evaluate(prev_model)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e666effd-373f-4389-895d-10aaffdbf9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 306      |\n",
      "|    ep_rew_mean     | 227      |\n",
      "| time/              |          |\n",
      "|    fps             | 1817     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 9        |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecFrameStack\n",
    "\n",
    "# Assuming you have a vectorized environment setup\n",
    "vec_env = make_vec_env('LunarLander-v2', n_envs=16)\n",
    "\n",
    "prev_model.learn(total_timesteps=3000000)\n",
    "\n",
    "prev_model.save('models/ppo-lander-3m')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
