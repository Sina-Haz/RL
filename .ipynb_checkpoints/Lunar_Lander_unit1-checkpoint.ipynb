{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7a2a713-3ad3-4833-8c5a-004e24fe9a7a",
   "metadata": {},
   "source": [
    "### Unit 1 Assignment\n",
    "\n",
    "Train an agent for the Lunar-Landerv2 environment using Stable-Baselines3 library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62e1fb80-4c00-4669-964d-d8cf47c70ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium\n",
    "\n",
    "from huggingface_sb3 import load_from_hub, package_to_hub\n",
    "from huggingface_hub import notebook_login # To log to our Hugging Face account to be able to upload models to the Hub.\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b522d921-7508-4a10-a6d2-aa43a3a3544d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action taken: 0\n",
      "Action taken: 0\n",
      "Action taken: 2\n",
      "Action taken: 0\n",
      "Action taken: 1\n",
      "Action taken: 1\n",
      "Action taken: 1\n",
      "Action taken: 0\n",
      "Action taken: 3\n",
      "Action taken: 0\n",
      "Action taken: 2\n",
      "Action taken: 0\n",
      "Action taken: 2\n",
      "Action taken: 1\n",
      "Action taken: 2\n",
      "Action taken: 0\n",
      "Action taken: 0\n",
      "Action taken: 2\n",
      "Action taken: 2\n",
      "Action taken: 0\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# First, we create our environment called LunarLander-v2\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "# Then we reset this environment\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(20):\n",
    "  # Take a random action\n",
    "  action = env.action_space.sample()\n",
    "  print(\"Action taken:\", action)\n",
    "\n",
    "  # Do this action in the environment and get\n",
    "  # next_state, reward, terminated, truncated and info\n",
    "  observation, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "  # If the game is terminated (in our case we land, crashed) or truncated (timeout)\n",
    "  if terminated or truncated:\n",
    "      # Reset the environment\n",
    "      print(\"Environment is reset\")\n",
    "      observation, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5fb36ea7-1df9-4819-9ab4-7ce452787097",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_____OBSERVATION SPACE_____ \n",
      "\n",
      "Observation Space Shape (8,)\n",
      "Sample observation [ 41.28432    -65.80316      2.3966308    4.271091     2.794731\n",
      "   1.7507254    0.24629244   0.07193787]\n"
     ]
    }
   ],
   "source": [
    "# We create our environment with gym.make(\"<name_of_the_environment>\")\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "env.reset()\n",
    "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
    "print(\"Observation Space Shape\", env.observation_space.shape)\n",
    "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c8566d-47b8-415b-bb81-262ebea0fc2b",
   "metadata": {},
   "source": [
    "We see with Observation Space Shape (8,) that the observation is a vector of size 8, where each value contains different information about the lander:\n",
    "-  Horizontal pad coordinate (x)\n",
    "- Vertical pad coordinate (y)\n",
    "- Horizontal speed (x)\n",
    "- Vertical speed (y)\n",
    "- Angle\n",
    "- Angular speed\n",
    "- If the left leg contact point has touched the land (boolean)\n",
    "- If the right leg contact point has touched the land (boolean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e46aabc0-8f12-4a9a-9993-4e4adebe374e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " _____ACTION SPACE_____ \n",
      "\n",
      "Action Space Shape 4\n",
      "Action Space Sample 0\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
    "print(\"Action Space Shape\", env.action_space.n)\n",
    "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e8c3c0-dd1e-4b39-8d7e-8580c4bdc22f",
   "metadata": {},
   "source": [
    "The action space (the set of possible actions the agent can take) is discrete with 4 actions available ğŸ®:\n",
    "\n",
    "- Action 0: Do nothing,\n",
    "- Action 1: Fire left orientation engine,\n",
    "- Action 2: Fire the main engine,\n",
    "- Action 3: Fire right orientation engine.\n",
    "\n",
    "Reward function (the function that will give a reward at each timestep) ğŸ’°:\n",
    "\n",
    "After every step a reward is granted. The total reward of an episode is the **sum of the rewards for all the steps within that episode**.\n",
    "\n",
    "For each step, the reward:\n",
    "\n",
    "- Is increased/decreased the closer/further the lander is to the landing pad.\n",
    "-  Is increased/decreased the slower/faster the lander is moving.\n",
    "- Is decreased the more the lander is tilted (angle not horizontal).\n",
    "- Is increased by 10 points for each leg that is in contact with the ground.\n",
    "- Is decreased by 0.03 points each frame a side engine is firing.\n",
    "- Is decreased by 0.3 points each frame the main engine is firing.\n",
    "\n",
    "The episode receive an **additional reward of -100 or +100 points for crashing or landing safely respectively.**\n",
    "\n",
    "An episode is **considered a solution if it scores at least 200 points.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2718792a-3096-44c9-9f95-148ddcedcda4",
   "metadata": {},
   "source": [
    "#### Vectorized Environment\n",
    "\n",
    "- We create a vectorized environment (a method for stacking multiple independent environments into a single environment) of 16 environments, this way, **we'll have more diverse experiences during the training.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a1251a7-6e96-4d97-85b4-f5fcc55dbafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "env = make_vec_env('LunarLander-v2', n_envs=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9463a42f-4422-496c-ae49-ec9024729255",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mPPO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpolicy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mType\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstable_baselines3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mActorCriticPolicy\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0menv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgymnasium\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstable_baselines3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvec_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_vec_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVecEnv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlearning_rate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0003\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_steps\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_epochs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mgamma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mgae_lambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.95\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mclip_range\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mclip_range_vf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mNoneType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mnormalize_advantage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0ment_coef\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mvf_coef\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0muse_sde\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msde_sample_freq\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtarget_kl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mstats_window_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtensorboard_log\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpolicy_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdevice\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'auto'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0m_init_setup_model\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "Proximal Policy Optimization algorithm (PPO) (clip version)\n",
       "\n",
       "Paper: https://arxiv.org/abs/1707.06347\n",
       "Code: This implementation borrows code from OpenAI Spinning Up (https://github.com/openai/spinningup/)\n",
       "https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail and\n",
       "Stable Baselines (PPO2 from https://github.com/hill-a/stable-baselines)\n",
       "\n",
       "Introduction to PPO: https://spinningup.openai.com/en/latest/algorithms/ppo.html\n",
       "\n",
       ":param policy: The policy model to use (MlpPolicy, CnnPolicy, ...)\n",
       ":param env: The environment to learn from (if registered in Gym, can be str)\n",
       ":param learning_rate: The learning rate, it can be a function\n",
       "    of the current progress remaining (from 1 to 0)\n",
       ":param n_steps: The number of steps to run for each environment per update\n",
       "    (i.e. rollout buffer size is n_steps * n_envs where n_envs is number of environment copies running in parallel)\n",
       "    NOTE: n_steps * n_envs must be greater than 1 (because of the advantage normalization)\n",
       "    See https://github.com/pytorch/pytorch/issues/29372\n",
       ":param batch_size: Minibatch size\n",
       ":param n_epochs: Number of epoch when optimizing the surrogate loss\n",
       ":param gamma: Discount factor\n",
       ":param gae_lambda: Factor for trade-off of bias vs variance for Generalized Advantage Estimator\n",
       ":param clip_range: Clipping parameter, it can be a function of the current progress\n",
       "    remaining (from 1 to 0).\n",
       ":param clip_range_vf: Clipping parameter for the value function,\n",
       "    it can be a function of the current progress remaining (from 1 to 0).\n",
       "    This is a parameter specific to the OpenAI implementation. If None is passed (default),\n",
       "    no clipping will be done on the value function.\n",
       "    IMPORTANT: this clipping depends on the reward scaling.\n",
       ":param normalize_advantage: Whether to normalize or not the advantage\n",
       ":param ent_coef: Entropy coefficient for the loss calculation\n",
       ":param vf_coef: Value function coefficient for the loss calculation\n",
       ":param max_grad_norm: The maximum value for the gradient clipping\n",
       ":param use_sde: Whether to use generalized State Dependent Exploration (gSDE)\n",
       "    instead of action noise exploration (default: False)\n",
       ":param sde_sample_freq: Sample a new noise matrix every n steps when using gSDE\n",
       "    Default: -1 (only sample at the beginning of the rollout)\n",
       ":param target_kl: Limit the KL divergence between updates,\n",
       "    because the clipping is not enough to prevent large update\n",
       "    see issue #213 (cf https://github.com/hill-a/stable-baselines/issues/213)\n",
       "    By default, there is no limit on the kl div.\n",
       ":param stats_window_size: Window size for the rollout logging, specifying the number of episodes to average\n",
       "    the reported success rate, mean episode length, and mean reward over\n",
       ":param tensorboard_log: the log location for tensorboard (if None, no logging)\n",
       ":param policy_kwargs: additional arguments to be passed to the policy on creation\n",
       ":param verbose: Verbosity level: 0 for no output, 1 for info messages (such as device or wrappers used), 2 for\n",
       "    debug messages\n",
       ":param seed: Seed for the pseudo random generators\n",
       ":param device: Device (cpu, cuda, ...) on which the code should be run.\n",
       "    Setting it to auto, the code will be run on the GPU if possible.\n",
       ":param _init_setup_model: Whether or not to build the network at the creation of the instance\n",
       "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/envs/pkgenv/lib/python3.9/site-packages/stable_baselines3/ppo/ppo.py\n",
       "\u001b[0;31mType:\u001b[0m           ABCMeta\n",
       "\u001b[0;31mSubclasses:\u001b[0m     "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe713334-8b6b-48b6-a2b0-f331222a7b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af2c2c60-bd3c-4a04-be61-03c3e16d3516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# Instantiate MlpPolicy model so that it works with vectorized environment env\n",
    "model = PPO(\n",
    "    policy = 'MlpPolicy',\n",
    "    env = env,\n",
    "    n_steps = 1024,\n",
    "    batch_size = 64,\n",
    "    n_epochs = 4,\n",
    "    gamma = 0.9995,\n",
    "    gae_lambda = 0.98,\n",
    "    ent_coef = 0.01,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f681f7a5-2199-47f0-b441-c27b21994796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 1e+03    |\n",
      "|    ep_rew_mean     | 133      |\n",
      "| time/              |          |\n",
      "|    fps             | 1051     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 15       |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 955         |\n",
      "|    ep_rew_mean          | 127         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 825         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 39          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006840083 |\n",
      "|    clip_fraction        | 0.0391      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.947      |\n",
      "|    explained_variance   | 0.9818963   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.1         |\n",
      "|    n_updates            | 124         |\n",
      "|    policy_gradient_loss | -0.00164    |\n",
      "|    value_loss           | 17.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 940          |\n",
      "|    ep_rew_mean          | 128          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 748          |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 65           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0047344733 |\n",
      "|    clip_fraction        | 0.0456       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.956       |\n",
      "|    explained_variance   | 0.96245944   |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.07         |\n",
      "|    n_updates            | 128          |\n",
      "|    policy_gradient_loss | -0.00186     |\n",
      "|    value_loss           | 44.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 923         |\n",
      "|    ep_rew_mean          | 125         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 704         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 93          |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004098669 |\n",
      "|    clip_fraction        | 0.0352      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.933      |\n",
      "|    explained_variance   | 0.9635301   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.8        |\n",
      "|    n_updates            | 132         |\n",
      "|    policy_gradient_loss | 0.000287    |\n",
      "|    value_loss           | 36.9        |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model_name = 'ppo-Lunar-Lander-v2'\n",
    "\n",
    "# do a small amount of training and run it to see how well we do\n",
    "model.learn(total_timesteps=50000);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77bb5325-cdfb-420a-8bbf-e069b2ac1652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating our model in a new, not vectorized environment\n",
    "\n",
    "# We wrap the environment in a monitor so it can be displayed\n",
    "eval_env = Monitor(gym.make(\"LunarLander-v2\", render_mode='rgb_array'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d599f2cb-d690-46bd-b62c-da575d720189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96713984-3673-4bdd-8b2a-9b849908f03b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9170e8c7-839a-4ad2-bf56-374b00820498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward=196.40 +/- 72.90873988792255\n"
     ]
    }
   ],
   "source": [
    "# Stable-Baselines3 provides function evaluate_policy to check how well our model performs\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e418e423-766a-455a-887f-050e066e0eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('ppo-lander-curr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa3b61a8-72f4-4cec-9c09-66d07c9a9c7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13efbad0d98f4b4198084df73432e2da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ppo-LunarLander-v2.zip:   0%|          | 0.00/148k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We can compare by loading our previous model:\n",
    "from huggingface_sb3 import load_from_hub\n",
    "checkpoint = load_from_hub(\n",
    "\trepo_id=\"shazeghi/ppo-LunarLander-v2\",\n",
    "\tfilename=\"ppo-LunarLander-v2.zip\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "178f9c4d-7c97-4a40-af71-e1cf0232ed17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f774c16fa60>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_model = PPO.load(checkpoint)\n",
    "prev_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f720dce4-b1dd-492c-be8e-d0d2f11009e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward=201.24 +/- 27.935596324140814\n",
      "mean_reward=233.66 +/- 14.130426487191404\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, env=eval_env):\n",
    "    mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "    print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")\n",
    "\n",
    "evaluate(model)\n",
    "evaluate(prev_model)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1344a4e0-cc60-4961-a2ea-69a8562c7765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: stable-baselines3[extra] in /common/home/skh79/.local/lib/python3.10/site-packages (2.0.0a5)\n",
      "Requirement already satisfied: gymnasium==0.28.1 in /common/home/skh79/.local/lib/python3.10/site-packages (from stable-baselines3[extra]) (0.28.1)\n",
      "Requirement already satisfied: numpy in /common/home/skh79/.local/lib/python3.10/site-packages (from stable-baselines3[extra]) (2.0.0)\n",
      "Requirement already satisfied: torch>=1.11 in /common/home/skh79/.local/lib/python3.10/site-packages (from stable-baselines3[extra]) (2.3.1)\n",
      "Requirement already satisfied: cloudpickle in /common/home/skh79/.local/lib/python3.10/site-packages (from stable-baselines3[extra]) (3.0.0)\n",
      "Requirement already satisfied: pandas in /common/home/skh79/.local/lib/python3.10/site-packages (from stable-baselines3[extra]) (2.2.2)\n",
      "Requirement already satisfied: matplotlib in /usr/lib/python3/dist-packages (from stable-baselines3[extra]) (3.5.1)\n",
      "Collecting opencv-python (from stable-baselines3[extra])\n",
      "  Downloading opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting tensorboard>=2.9.1 (from stable-baselines3[extra])\n",
      "  Downloading tensorboard-2.17.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: psutil in /usr/lib/python3/dist-packages (from stable-baselines3[extra]) (5.9.0)\n",
      "Requirement already satisfied: tqdm in /common/home/skh79/.local/lib/python3.10/site-packages (from stable-baselines3[extra]) (4.66.4)\n",
      "Collecting rich (from stable-baselines3[extra])\n",
      "  Using cached rich-13.7.1-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting shimmy~=0.2.1 (from shimmy[atari]~=0.2.1; extra == \"extra\"->stable-baselines3[extra])\n",
      "  Downloading Shimmy-0.2.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: pillow in /usr/lib/python3/dist-packages (from stable-baselines3[extra]) (9.0.1)\n",
      "Collecting autorom~=0.6.0 (from autorom[accept-rom-license]~=0.6.0; extra == \"extra\"->stable-baselines3[extra])\n",
      "  Downloading AutoROM-0.6.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: pygame in /common/home/skh79/.local/lib/python3.10/site-packages (from stable-baselines3[extra]) (2.1.3)\n",
      "Requirement already satisfied: jax-jumpy>=1.0.0 in /common/home/skh79/.local/lib/python3.10/site-packages (from gymnasium==0.28.1->stable-baselines3[extra]) (1.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /common/home/skh79/.local/lib/python3.10/site-packages (from gymnasium==0.28.1->stable-baselines3[extra]) (4.12.2)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /common/home/skh79/.local/lib/python3.10/site-packages (from gymnasium==0.28.1->stable-baselines3[extra]) (0.0.4)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from autorom~=0.6.0->autorom[accept-rom-license]~=0.6.0; extra == \"extra\"->stable-baselines3[extra]) (8.0.3)\n",
      "Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from autorom~=0.6.0->autorom[accept-rom-license]~=0.6.0; extra == \"extra\"->stable-baselines3[extra]) (2.25.1)\n",
      "Collecting AutoROM.accept-rom-license (from autorom[accept-rom-license]~=0.6.0; extra == \"extra\"->stable-baselines3[extra])\n",
      "  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting ale-py~=0.8.1 (from shimmy[atari]~=0.2.1; extra == \"extra\"->stable-baselines3[extra])\n",
      "  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard>=2.9.1->stable-baselines3[extra])\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard>=2.9.1->stable-baselines3[extra])\n",
      "  Downloading grpcio-1.64.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.3 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard>=2.9.1->stable-baselines3[extra])\n",
      "  Downloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting protobuf!=4.24.0,<5.0.0,>=3.19.6 (from tensorboard>=2.9.1->stable-baselines3[extra])\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/lib/python3/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (59.6.0)\n",
      "Requirement already satisfied: six>1.9 in /usr/lib/python3/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.16.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard>=2.9.1->stable-baselines3[extra])\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard>=2.9.1->stable-baselines3[extra])\n",
      "  Downloading werkzeug-3.0.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from torch>=1.11->stable-baselines3[extra]) (3.6.0)\n",
      "Requirement already satisfied: sympy in /usr/lib/python3/dist-packages (from torch>=1.11->stable-baselines3[extra]) (1.9)\n",
      "Requirement already satisfied: networkx in /common/home/skh79/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3[extra]) (3.3)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch>=1.11->stable-baselines3[extra]) (3.0.3)\n",
      "Requirement already satisfied: fsspec in /common/home/skh79/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3[extra]) (2024.6.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /common/home/skh79/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3[extra]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /common/home/skh79/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3[extra]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /common/home/skh79/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3[extra]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /common/home/skh79/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3[extra]) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /common/home/skh79/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3[extra]) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /common/home/skh79/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3[extra]) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /common/home/skh79/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3[extra]) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /common/home/skh79/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3[extra]) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /common/home/skh79/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3[extra]) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /common/home/skh79/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3[extra]) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /common/home/skh79/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3[extra]) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /common/home/skh79/.local/lib/python3.10/site-packages (from torch>=1.11->stable-baselines3[extra]) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /common/home/skh79/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11->stable-baselines3[extra]) (12.5.82)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /common/home/skh79/.local/lib/python3.10/site-packages (from pandas->stable-baselines3[extra]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->stable-baselines3[extra]) (2022.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /common/home/skh79/.local/lib/python3.10/site-packages (from pandas->stable-baselines3[extra]) (2024.1)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->stable-baselines3[extra])\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting pygments<3.0.0,>=2.13.0 (from rich->stable-baselines3[extra])\n",
      "  Using cached pygments-2.18.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting importlib-resources (from ale-py~=0.8.1->shimmy[atari]~=0.2.1; extra == \"extra\"->stable-baselines3[extra])\n",
      "  Downloading importlib_resources-6.4.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra])\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra])\n",
      "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Downloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\n",
      "Downloading Shimmy-0.2.1-py3-none-any.whl (25 kB)\n",
      "Downloading tensorboard-2.17.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached rich-13.7.1-py3-none-any.whl (240 kB)\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.64.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached pygments-2.18.0-py3-none-any.whl (1.2 MB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m227.3/227.3 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading importlib_resources-6.4.0-py3-none-any.whl (38 kB)\n",
      "Building wheels for collected packages: AutoROM.accept-rom-license\n",
      "  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446661 sha256=e889523d73355b1818818225834207466decf6a1d22dec761ebbc05472cba974\n",
      "  Stored in directory: /common/home/skh79/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\n",
      "Successfully built AutoROM.accept-rom-license\n",
      "\u001b[33mWARNING: Error parsing dependencies of send2trash: Expected matching RIGHT_PARENTHESIS for LEFT_PARENTHESIS, after version specifier\n",
      "    sys-platform (==\"darwin\") ; extra == 'objc'\n",
      "                 ~^\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: tensorboard-data-server, pygments, protobuf, opencv-python, mdurl, MarkupSafe, markdown, importlib-resources, grpcio, AutoROM.accept-rom-license, autorom, absl-py, werkzeug, markdown-it-py, ale-py, tensorboard, shimmy, rich\n",
      "Successfully installed AutoROM.accept-rom-license-0.6.1 MarkupSafe-2.1.5 absl-py-2.1.0 ale-py-0.8.1 autorom-0.6.1 grpcio-1.64.1 importlib-resources-6.4.0 markdown-3.6 markdown-it-py-3.0.0 mdurl-0.1.2 opencv-python-4.10.0.84 protobuf-4.25.3 pygments-2.18.0 rich-13.7.1 shimmy-0.2.1 tensorboard-2.17.0 tensorboard-data-server-0.7.2 werkzeug-3.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install stable-baselines3[extra]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e666effd-373f-4389-895d-10aaffdbf9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 772      |\n",
      "|    ep_rew_mean     | 105      |\n",
      "| time/              |          |\n",
      "|    fps             | 947      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 17       |\n",
      "|    total_timesteps | 16384    |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f778af6efd0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev_model.n_envs = 16\n",
    "prev_model.set_env(env)\n",
    "\n",
    "# prev_model.learn(total_timesteps=10000)\n",
    "\n",
    "model.learn(total_timesteps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37b8a6f5-bfa9-498f-b6a1-5ed15ddef779",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a37866768cd2491599c2b3e305eb4af2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()\n",
    "!git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be073c9-4788-413b-99f0-a74e913fe239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aa712f80-9712-4fb3-9db9-ff59f3aa26af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting moviepy\n",
      "  Downloading moviepy-1.0.3.tar.gz (388 kB)\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m388.3/388.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: decorator<5.0,>=4.0.2 in /usr/lib/python3/dist-packages (from moviepy) (4.4.2)\n",
      "Collecting proglog<=1.0.0 (from moviepy)\n",
      "  Downloading proglog-0.1.10-py3-none-any.whl.metadata (639 bytes)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/lib/python3/dist-packages (from moviepy) (2.25.1)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /common/home/skh79/.local/lib/python3.10/site-packages (from moviepy) (4.66.4)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /common/home/skh79/.local/lib/python3.10/site-packages (from moviepy) (2.0.0)\n",
      "Collecting imageio<3.0,>=2.5 (from moviepy)\n",
      "  Downloading imageio-2.34.2-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting imageio_ffmpeg>=0.2.0 (from moviepy)\n",
      "  Downloading imageio_ffmpeg-0.5.1-py3-none-manylinux2010_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: pillow>=8.3.2 in /usr/lib/python3/dist-packages (from imageio<3.0,>=2.5->moviepy) (9.0.1)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from imageio_ffmpeg>=0.2.0->moviepy) (59.6.0)\n",
      "Downloading imageio-2.34.2-py3-none-any.whl (313 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m313.5/313.5 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading imageio_ffmpeg-0.5.1-py3-none-manylinux2010_x86_64.whl (26.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m26.9/26.9 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading proglog-0.1.10-py3-none-any.whl (6.1 kB)\n",
      "Building wheels for collected packages: moviepy\n",
      "  Building wheel for moviepy (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for moviepy: filename=moviepy-1.0.3-py3-none-any.whl size=110747 sha256=afaa445429ebdd2d87b9c27de7f8da1b97c5e134e5f95e2c935f14d825f02d79\n",
      "  Stored in directory: /common/home/skh79/.cache/pip/wheels/96/32/2d/e10123bd88fbfc02fed53cc18c80a171d3c87479ed845fa7c1\n",
      "Successfully built moviepy\n",
      "\u001b[33mWARNING: Error parsing dependencies of send2trash: Expected matching RIGHT_PARENTHESIS for LEFT_PARENTHESIS, after version specifier\n",
      "    sys-platform (==\"darwin\") ; extra == 'objc'\n",
      "                 ~^\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: proglog, imageio_ffmpeg, imageio, moviepy\n",
      "Successfully installed imageio-2.34.2 imageio_ffmpeg-0.5.1 moviepy-1.0.3 proglog-0.1.10\n"
     ]
    }
   ],
   "source": [
    "!pip install moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1095b8db-c547-485d-89a6-4d819b738352",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1b097319-7ba9-4b83-b41e-97acac6be5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mâ„¹ This function will save, evaluate, generate a video of your agent,\n",
      "create a model card and push everything to the hub. It might take up to 1min.\n",
      "This is a work in progress: if you encounter a bug, please open an issue.\u001b[0m\n"
     ]
    },
    {
     "ename": "DependencyNotInstalled",
     "evalue": "moviepy is not installed, run `pip install moviepy`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/pkgenv/lib/python3.9/site-packages/gymnasium/wrappers/monitoring/video_recorder.py:45\u001b[0m, in \u001b[0;36mVideoRecorder.__init__\u001b[0;34m(self, env, path, metadata, enabled, base_path, disable_logger)\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# check that moviepy is now installed\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmoviepy\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'moviepy'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m eval_env \u001b[38;5;241m=\u001b[39m DummyVecEnv([\u001b[38;5;28;01mlambda\u001b[39;00m: gym\u001b[38;5;241m.\u001b[39mmake(env_id, render_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrgb_array\u001b[39m\u001b[38;5;124m\"\u001b[39m)])\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# PLACE the package_to_hub function you've just filled here\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[43mpackage_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprev_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Our trained model\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[43m               \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# The name of our trained model\u001b[39;49;00m\n\u001b[1;32m     30\u001b[0m \u001b[43m               \u001b[49m\u001b[43mmodel_architecture\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_architecture\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# The model architecture we used: in our case PPO\u001b[39;49;00m\n\u001b[1;32m     31\u001b[0m \u001b[43m               \u001b[49m\u001b[43menv_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Name of the environment\u001b[39;49;00m\n\u001b[1;32m     32\u001b[0m \u001b[43m               \u001b[49m\u001b[43meval_env\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Evaluation Environment\u001b[39;49;00m\n\u001b[1;32m     33\u001b[0m \u001b[43m               \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\u001b[39;49;00m\n\u001b[1;32m     34\u001b[0m \u001b[43m               \u001b[49m\u001b[43mcommit_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_message\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pkgenv/lib/python3.9/site-packages/huggingface_sb3/push_to_hub.py:382\u001b[0m, in \u001b[0;36mpackage_to_hub\u001b[0;34m(model, model_name, model_architecture, env_id, eval_env, repo_id, commit_message, is_deterministic, n_eval_episodes, token, video_length, logs)\u001b[0m\n\u001b[1;32m    377\u001b[0m mean_reward, std_reward \u001b[38;5;241m=\u001b[39m _evaluate_agent(\n\u001b[1;32m    378\u001b[0m     model, eval_env, n_eval_episodes, is_deterministic, tmpdirname\n\u001b[1;32m    379\u001b[0m )\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# Step 4: Generate a video\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m \u001b[43m_generate_replay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_deterministic\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtmpdirname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;66;03m# Step 5: Generate the model card\u001b[39;00m\n\u001b[1;32m    385\u001b[0m generated_model_card, metadata \u001b[38;5;241m=\u001b[39m _generate_model_card(\n\u001b[1;32m    386\u001b[0m     model_architecture, env_id, mean_reward, std_reward\n\u001b[1;32m    387\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/pkgenv/lib/python3.9/site-packages/huggingface_sb3/push_to_hub.py:141\u001b[0m, in \u001b[0;36m_generate_replay\u001b[0;34m(model, eval_env, video_length, is_deterministic, local_path)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tempfile\u001b[38;5;241m.\u001b[39mTemporaryDirectory() \u001b[38;5;28;01mas\u001b[39;00m tmpdirname:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# Step 1: Create the VecVideoRecorder\u001b[39;00m\n\u001b[1;32m    133\u001b[0m     env \u001b[38;5;241m=\u001b[39m VecVideoRecorder(\n\u001b[1;32m    134\u001b[0m         eval_env,\n\u001b[1;32m    135\u001b[0m         tmpdirname,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    138\u001b[0m         name_prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    139\u001b[0m     )\n\u001b[0;32m--> 141\u001b[0m     obs \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m     lstm_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    143\u001b[0m     episode_starts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((env\u001b[38;5;241m.\u001b[39mnum_envs,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pkgenv/lib/python3.9/site-packages/stable_baselines3/common/vec_env/vec_video_recorder.py:68\u001b[0m, in \u001b[0;36mVecVideoRecorder.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvObs:\n\u001b[1;32m     67\u001b[0m     obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvenv\u001b[38;5;241m.\u001b[39mreset()\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_video_recorder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obs\n",
      "File \u001b[0;32m~/miniconda3/envs/pkgenv/lib/python3.9/site-packages/stable_baselines3/common/vec_env/vec_video_recorder.py:76\u001b[0m, in \u001b[0;36mVecVideoRecorder.start_video_recorder\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m video_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-step-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-to-step-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_id\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m base_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_folder, video_name)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_recorder \u001b[38;5;241m=\u001b[39m \u001b[43mvideo_recorder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mVideoRecorder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[43m    \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstep_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_id\u001b[49m\u001b[43m}\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_recorder\u001b[38;5;241m.\u001b[39mcapture_frame()\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecorded_frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/pkgenv/lib/python3.9/site-packages/gymnasium/wrappers/monitoring/video_recorder.py:47\u001b[0m, in \u001b[0;36mVideoRecorder.__init__\u001b[0;34m(self, env, path, metadata, enabled, base_path, disable_logger)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmoviepy\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error\u001b[38;5;241m.\u001b[39mDependencyNotInstalled(\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmoviepy is not installed, run `pip install moviepy`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     49\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_async \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msemantics.async\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menabled \u001b[38;5;241m=\u001b[39m enabled\n",
      "\u001b[0;31mDependencyNotInstalled\u001b[0m: moviepy is not installed, run `pip install moviepy`"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "from huggingface_sb3 import package_to_hub\n",
    "\n",
    "# PLACE the variables you've just defined two cells above\n",
    "# Define the name of the environment\n",
    "env_id = \"LunarLander-v2\"\n",
    "\n",
    "# TODO: Define the model architecture we used\n",
    "model_architecture = \"PPO\"\n",
    "\n",
    "## Define a repo_id\n",
    "## repo_id is the id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n",
    "## CHANGE WITH YOUR REPO ID\n",
    "repo_id = \"shazeghi/ppo-LunarLander-v2\" # Change with your repo id, you can't push with mine ğŸ˜„\n",
    "\n",
    "## Define the commit message\n",
    "commit_message = \"Upload PPO LunarLander-v2 trained agent\"\n",
    "\n",
    "# Create the evaluation env and set the render_mode=\"rgb_array\"\n",
    "eval_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n",
    "\n",
    "# PLACE the package_to_hub function you've just filled here\n",
    "package_to_hub(model=prev_model, # Our trained model\n",
    "               model_name=model_name, # The name of our trained model\n",
    "               model_architecture=model_architecture, # The model architecture we used: in our case PPO\n",
    "               env_id=env_id, # Name of the environment\n",
    "               eval_env=eval_env, # Evaluation Environment\n",
    "               repo_id=repo_id, # id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n",
    "               commit_message=commit_message)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
